\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{cancel}

\DeclareMathOperator{\given}{\mid}

\begin{document}

	\section*{HW3}

	\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill} } lll}
		Jimmy Hold\"{o} & & Jared Karr\\
		890130-6319 & & 801120-4693\\
		\it{gusholji@student.gu.se} & & \it{karr@student.chalmers.se}\\
	\end{tabular*}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Theoretical problems}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Topological properties}
	\paragraph{(a)}
	 The network that can be trained by backpropagation is network number 1.
	\paragraph{(b)}
	The topological properties a network must fulfill are that information only moves in one direction, forward, there are no cycles and loops and it requiers that the activiation function used by the neurons be differentiable.

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Committee}
	The activation function for the output is linear:
	\begin{align*}
		y = g(\bm{w}^T\bm{x}) = c \bm{w}^T \bm{x}.
	\end{align*}

	\noindent The two networks combined:

	\begin{align*}
	 y &=g(\frac{1}{2} \bm{w}_1^T \bm{x}+\frac{1}{2} \bm{w}_2^{T} \bm{x})\\
	 &= \frac{1}{2}g(\bm{w}_1^T \bm{x})+\frac{1}{2}g(\bm{w}_2^T \bm{x})\\
	 &=	\frac{c}{2}\bm{w}_1^T\bm{x}+\frac{c}{2}\bm{w}_2^T\bm{x} \\
	 &= \frac{c}{2}(\bm{w}_1^T+\bm{w}_2^T)\bm{x}\\
	 &= c \bm{w}_3^T \bm{x} = g(\bm{w}_3^T\bm{x})
	\end{align*}

	\noindent Weights for the new network:
	\begin{align*}
		\bm{w}_3^T=\frac{1}{2}(\bm{w}_1^T+\bm{w}_2^T)
	\end{align*}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Backpropagation - shallow network}
	The update to weight $w_i$ is
	\begin{align*}
		\Delta w_i &= -\lambda\frac{\partial E}{\partial w_i} \\
		&= -\lambda \frac{\partial}{\partial w_i}\frac{1}{2}(t-y)^2\\
		&= -\lambda (t-y)\frac{\partial y}{\partial w_i}\\
		&= -\lambda (t-y)x_i
	\end{align*}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Backpropagation}
	\paragraph{(a)}
	\begin{align*}
		\frac{\partial E}{\partial z_k} &= \frac{\partial y_k}{\partial z_k}\cdot\frac{\partial E}{\partial y_k}\\
		&=g'(z_k)\frac{\partial E}{\partial y_k}\\
\intertext{\textbf{(b)}}
		\frac{\partial E}{\partial z_j} &= \frac{\partial y_j}{\partial z_j}\cdot\frac{\partial E}{\partial y_j}\\
		&=g'(z_j)\sum_k\frac{\partial z_k}{\partial y_j}\cdot\frac{\partial E}{\partial z_k}\\
		&=g'(z_j)\sum_kw_{jk}g'(z_k)\frac{\partial E}{\partial y_k}\\
\intertext{\textbf{(c)}}
		\frac{\partial E}{\partial w_{jk}} &= \frac{\partial z_k}{\partial w_{jk}}\cdot\frac{\partial E}{\partial z_k}\\
		&=y_jg'(z_k)\frac{\partial E}{\partial y_k}\\
\intertext{\textbf{(d)}}
		\frac{\partial E}{\partial w_{ij}} &= \frac{\partial z_j}{\partial w_{ij}}\cdot\frac{\partial E}{\partial z_j}\\
		&=y_ig'(z_j)\sum_kw_{jk}g'(z_k)\frac{\partial E}{\partial y_k}
	\end{align*}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Practical problems}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Backpropagation on paper}
  The network can be written as
  \begin{align*}
    \tag{input layer}
      x_i\\
    \tag{hidden layer weighting}
      z_j&=\sum_iw_{ji}x_i\\
    \tag{hidden layer activation}
      y_j&=\frac{1}{1+e^{-z_j}}\\
    \tag{output layer weighting}
      z_k&=\sum_jw_{kj}y_j\\
    \tag{output layer normalization}
      C&=\sum_ke^{z_k}\\
    \tag{output layer activation}
      y_k&=\frac{e^{z_k}}{C}\\
    \tag{output target}
      t_k
  \end{align*}
  and the error function as
  \begin{align*}
    \tag{classification error}
      E_c &= -\sum_kt_k\log y_k=-\sum_kt_k(z_k-\log C) \\
    \tag{weight decay error}
      E_d &= \frac{1}{2}\sum_{i,j}w_{ji}^2 + \frac{1}{2}\sum_{j,k}w_{kj}^2\\
    \tag{total error}
      E &= E_c + \alpha E_d.
  \end{align*}
  The relevant gradients for backpropagation are
  \begin{align*}
    \tag{output activation}
    \frac{\partial E}{\partial z_{k_0}} &= -t_{k_0}+(\sum_kt_k)\frac{\partial}{\partial z_{k_0}}\log C\\
      &= -t_{k_0}+\frac{1}{C}\frac{\partial}{\partial z_{k_0}}C\\
      &= -t_{k_0}+\frac{1}{C}e^{z_{k_0}} = -t_{k_0}+y_{k_0}\\
\intertext{}
    \tag{output weights}
    \frac{\partial E}{\partial w_{kj}}
      &= \frac{\partial E_c}{\partial z_k}\frac{\partial z_k}{\partial w_{kj}}+\frac{\partial E_d}{\partial w_{kj}}\\
      &= \left(y_k-t_k\right)\cdot y_j + \alpha w_{kj}
\intertext{}
    \tag{hidden output}
    \frac{\partial E}{\partial y_j}
      &= \sum_k\left(\frac{\partial z_k}{\partial y_j}\cdot\frac{\partial E}{\partial z_k}\right)\\
      &= \sum_kw_{kj}\cdot(y_k-t_k)
\intertext{}
    \tag{hidden activation}
    \frac{\partial E}{\partial z_j}
      &= \frac{\partial y_j}{\partial z_j}\cdot\frac{\partial E}{\partial y_j}\\
      &= \left(\frac{1}{1+e^{-z_j}}\right)^2e^{-z_j}
        \cdot\sum_kw_{kj}(y_k-t_k)\\
      &= \frac{1}{1+e^{z_j}}\cdot\frac{e^{z_j}}{1+e^{z_j}}
        \cdot\sum_kw_{kj}(y_k-t_k)\\
      &= y_j(1-y_j)\cdot\sum_kw_{kj}(y_k-t_k)\\
\intertext{}
    \tag{hidden weights}
    \frac{\partial E}{\partial w_{ji}}
      &= \frac{\partial E_d}{\partial w_{ji}} + \frac{\partial z_j}{\partial w_{ji}}\cdot\frac{\partial E_c}{\partial z_j}\\
      &= \alpha w_{ji} + x_i\cdot y_j(1-y_j)\sum_kw_{kj}(y_k-t_k)
  \end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Backpropagation}

The gradient is computed using the following code:
\begin{verbatim}
[~,N] = size(data.inputs);
wdpart.input_to_hid = model.input_to_hid * wd_coefficient;
wdpart.hid_to_class = model.hid_to_class * wd_coefficient;

dEd_class_input = (class_prob - data.targets) / N;
dEd_hid_output = model.hid_to_class' * dEd_class_input;
dEd_hid_input = hid_output .* (1 - hid_output) .* dEd_hid_output;

classpart.hid_to_class = dEd_class_input * hid_output';
classpart.input_to_hid = dEd_hid_input * data.inputs';

res.input_to_hid = classpart.input_to_hid + wdpart.input_to_hid;
res.hid_to_class = classpart.hid_to_class + wdpart.hid_to_class;
\end{verbatim}
The training cost when running \texttt{net(0.1, 7, 10, 0, 0, false, 4)} is 2.76838.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimization}

The best results were achieved at learning rate $\lambda=0.2$ with momentum.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalization}
\paragraph{(a)}
The validation cost of \texttt{net(0, 200, 1000, 0.35, 0.9, false, 100)} is 0.43019.
\paragraph{(b)}
The validation cost of \texttt{net(0, 200, 1000, 0.35, 0.9, true, 100)} is 0.33451.
\paragraph{(c)}
The best result was achieved with weight decay $\alpha=0.001$, having a validation cost of 0.28791.
\paragraph{(d)}
The best result was achieved with 30 hidden units, having a validation cost of 0.31708.
\paragraph{(e)}
With early stopping, the best result was achieved with 37 hidden units, having a validation cost of 0.26517.
\paragraph{(f)}
The classification error rate for \texttt{net(0, 37, 1000, 0.35, 0.9, true, 100)} was 8.1\%.







\end{document}
