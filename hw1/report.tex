\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{epstopdf}

\DeclareMathOperator{\given}{\mid}

\begin{document}

\section*{HW1}

\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill} } lll}
Jimmy Hold\"{o} & & Jared Karr\\
890130-6319 & & 801120-4693\\
\it{gusholji@student.gu.se} & & \it{karr@student.chalmers.se}\\
\end{tabular*}

\section{Theoretical problems}
\subsection{Maximum likelihood estimator (MLE)}
The likelihood of a set of observations $\mathbf{x_1},\dots,\mathbf{x_n}\in\mathbb{R}^p$ drawn I.I.D. from a $p$-dimensional spherical Gaussian distribution having mean $\boldsymbol\mu\in\mathbb{R}^p$ and variance $\sigma^2\in\mathbb{R}_{>0}$ is
\begin{align*}
  P(\mathbf{x_1},\dots,\mathbf{x_n}\given \boldsymbol\mu,\sigma^2)
    &=\prod_{i=1}^n\mathcal{N}(\mathbf{x_i}\given \boldsymbol\mu, \sigma^2)\\
    &=\prod_{i=1}^n\left(
        \frac{1}{\sqrt{2\pi}\sigma}
      \right)^p\exp\left(
        \frac{-(\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)}
             {2\sigma^2}
      \right).
\end{align*}
To find $\sigma_\mathrm{\scriptscriptstyle{MLE}}$, it is easier to maximize the log-likelihood
\begin{equation*}
  \log P(\mathbf{x_1},\dots,\mathbf{x_n}\given\boldsymbol\mu,\sigma^2)
    =\sum_{i=1}^n
      -p\log\sqrt{2\pi}
      -p\log\sigma
      -\frac{(\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)}{2\sigma^2} 
\end{equation*}
with respect to $\sigma$. Taking the appropriate partial derivative and setting to zero,
\begin{align*}
\frac{\partial\log P}{\partial\sigma}
  &=\sum_{i=1}^n
    -\frac{p}{\sigma}
    +\frac{1}{\sigma^3}\cdot(\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)\\
  &=-\frac{np}{\sigma}
    +\frac{1}{\sigma^3}\sum_{i=1}^n(\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)=0\\
np\sigma^2
  &=\sum_{i=1}^n(\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)\\
\sigma_\mathrm{\scriptscriptstyle{MLE}}&=\sqrt{\frac{1}{np}\sum_{i=1}^n(\mathbf{x_i}-\boldsymbol\mu_\mathrm{\scriptscriptstyle{MLE}})^\top(\mathbf{x_i}-\boldsymbol\mu_\mathrm{\scriptscriptstyle{MLE}})}.
\end{align*}

\subsection{Posterior distributions}

\paragraph{(a)} Bayes rule gives the posterior distribution
\begin{align*}
  P(s\given \mathbf{x_1},\dots,\mathbf{x_n};\alpha,\beta)
    &=\frac{
        P(\mathbf{x_1},\dots,\mathbf{x_n}\given s)P(s\given\alpha,\beta)
      }{
        \int_{s=0}^\infty P(\mathbf{x_1},\dots,\mathbf{x_n}\given s)P(s\given\alpha,\beta)\,ds
      },
\end{align*}
but we can ignore the normalization constants of each distribution to simplify the expression:
\begin{align*}
  P(s\given \mathbf{x_1},\dots,\mathbf{x_n};\alpha,\beta)
    &\propto P(\mathbf{x_1},\dots,\mathbf{x_n}\given s)P(s\given\alpha,\beta)\\
    &\propto
      \prod_{i=1}^n \left(
        s^{-1}\exp\left(-\frac{(\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)}{2s}\right)
      \right)
      \cdot
      s^{-\alpha-1}\exp\left(-\frac{\beta}{s}\right)\\
    &\propto
      s^{-n-\alpha-1}
      \exp\left(
        \frac{
          -\beta
          -\frac{1}{2}\left(
            \sum_{i=1}^n
              (\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)
          \right)
        }{
          s
        }
      \right)\\
    &\propto
      s^{-(\alpha+n)-1}
      \exp\left(
        \frac{
          -(\beta+n\sigma_\mathrm{\scriptscriptstyle{MLE}}^2)
        }{
          s
        }
      \right).
\end{align*}
Let $\alpha'=\alpha+n$ and $\beta'=\beta+n\sigma_\mathrm{\scriptscriptstyle{MLE}}^2$, then the posterior distribution takes the same form as the prior:
\begin{align*}
P(s\given \mathbf{x_1},\dots,\mathbf{x_n};\alpha,\beta)
  &=P(s\given\alpha',\beta')\\
  &=\frac{{\beta'}^{\alpha'}}{\Gamma(\alpha')}s^{-\alpha'-1}\exp\left(\frac{-\beta'}{s}\right)
\end{align*}

\paragraph{(b)} The Bayes factor between models $M_A$ and $M_B$ is defined to be
\begin{align*}
\mathrm{BF}&=
  \frac{
    P(\mathbf{x_1},\dots,\mathbf{x_n}\given\alpha_A,\beta_A)
  }{
    P(\mathbf{x_1},\dots,\mathbf{x_n}\given\alpha_B,\beta_B)
  }.
\end{align*}
From Bayes rule, we get
\begin{align*}
\mathrm{BF}&=
  \frac{
    \frac{
      P(\mathbf{x_1},\dots,\mathbf{x_n}\given s)P(s\given\alpha_A,\beta_A)      
    }{
      P(s\given\alpha_A',\beta_A')
    }
  }{
    \frac{
      P(\mathbf{x_1},\dots,\mathbf{x_n}\given s)P(s\given\alpha_B,\beta_B)      
    }{
      P(s\given\alpha_B',\beta_B')
    }
  }\\
&=
  \frac{
    P(s\given\alpha_A,\beta_A)P(s\given\alpha_B',\beta_B')
  }{
    P(s\given\alpha_A',\beta_A')P(s\given\alpha_B,\beta_B)
  }\\
&=
  \frac{
    \beta_A^{\alpha_A}\beta_B'^{\alpha_B'}
    \Gamma(\alpha_A')\Gamma(\alpha_B)
  }{
    \beta_A'^{\alpha_A'}\beta_B^{\alpha_B}
    \Gamma(\alpha_A)\Gamma(\alpha_B')
  }
  \cdot
  s^{\alpha_A'-\alpha_A+\alpha_B-\alpha_B'}
  \exp\left(
    \frac{
      \beta_A'-\beta_A+\beta_B-\beta_B'
    }{
      s
    }
  \right)
\end{align*}
Note that the posterior models $(\alpha_A', \beta_A')$ and $(\alpha_B', \beta_B')$ incorporate the same evidence, so that
\begin{align*}
  \alpha_A'-\alpha_A+\alpha_B-\alpha_B'&=
  \alpha_A+n-\alpha_A+\alpha_B-\alpha_B-n=0\\
  \intertext{and}
  \beta_A'-\beta_A+\beta_B-\beta_B'&=
  \beta_A+n\sigma_\mathrm{\scriptscriptstyle{MLE}}^2-\beta_A+\beta_B-\beta_B-n\sigma_\mathrm{\scriptscriptstyle{MLE}}^2=0,
\end{align*}
causing the terms involving $s$ to disappear from the Bayes factor:
\begin{align*}
\mathrm{BF}&=
  \frac{
    \beta_A^{\alpha_A}\beta_B'^{\alpha_B'}
    \Gamma(\alpha_A')\Gamma(\alpha_B)
  }{
    \beta_A'^{\alpha_A'}\beta_B^{\alpha_B}
    \Gamma(\alpha_A)\Gamma(\alpha_B')
  }
\end{align*}
\paragraph{(c)}
The maximum a posteriori estimate for $\sigma^2$ can be found by taking the partial derivative of the logarithm of the posterior model with respect to $\sigma^2$ and setting to zero:
\begin{align*}
\frac{\partial\log P(s\given\alpha'\beta')}{\partial s}
&=
  \frac{\partial}{\partial s}
    \alpha'\log\beta' - \log\Gamma(\alpha')
    -(\alpha'+1)\log s -\frac{\beta'}{s}\\
&=
  \frac{-(\alpha'+1)}{s} + \frac{\beta'}{s^2}=0
\\
s_\mathrm{\scriptscriptstyle{MAP}}&=\frac{\beta'}{\alpha'+1}.  
\end{align*}
The Bayes factor can be approximated then by
\begin{align*}
\mathrm{BF}&=
  \frac{
    P(\mathbf{x_1},\dots,\mathbf{x_n}\given s_A)
  }{
    P\mathbf{x_1},\dots,\mathbf{x_n}(\given s_B)
  }
\end{align*}
where $s_A=\frac{\beta_A}{\alpha_A+1}$ and $s_B=\frac{\beta_B}{\alpha_B+1}$ are the MAP estimates for $M_A$ and $M_B$ respectively. Thus,
\begin{align*}
\mathrm{BF}&=
  \frac{
    \prod_{i=1}^n
      \frac{1}{2\pi s_A}
      \exp\left(
        \frac{
          -(\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)
        }{
          2s_A
        }
      \right)
  }{
    \prod_{i=1}^n
      \frac{1}{2\pi s_B}
      \exp\left(
        \frac{
          -(\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)
        }{
          2s_B
        }
      \right)
  }
\\
&=
  \prod_{i=1}^n
    \frac{s_B}{s_A}
    \exp\left(
      (\frac{1}{s_B}-\frac{1}{s_A})
      (\frac{(\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)}{2})
    \right)
\\
&=
  \left(\frac{s_B}{s_A}\right)^n
  \exp\left(
    (\frac{1}{s_B}-\frac{1}{s_A})
    \cdot\sum_{i=1}^n\frac{(\mathbf{x_i}-\boldsymbol\mu)^\top(\mathbf{x_i}-\boldsymbol\mu)}{2}
  \right)
\\
&=
  \left(\frac{s_B}{s_A}\right)^n
  \exp\left(
    (\frac{1}{s_B}-\frac{1}{s_A})
    \cdot ns_\mathrm{\scriptscriptstyle{MLE}}
  \right).
\end{align*}
The Bayes factor indicates the relative agreement between each model's MAP estimate and the evidence's MLE estimate. Taking the partial derivative of the logarithm of the Bayes factor with respect to $s_A$ shows
\begin{align*}
\frac{\partial\log\mathrm{BF}}{\partial s_A}&=
  \frac{\partial}{\partial s_A}
  \left(
    n\log s_B - n\log s_A + \frac{ns_\mathrm{\scriptscriptstyle{MLE}}}{s_B}-\frac{ns_\mathrm{\scriptscriptstyle{MLE}}}{s_A}
  \right)
\\
&=
  \frac{-n}{s_A}+\frac{ns_\mathrm{\scriptscriptstyle{MLE}}}{s_A^2}=0
\\
s_A&=s_\mathrm{\scriptscriptstyle{MLE}}.  
\end{align*}
That is, the Bayes factor is maximized in favor of $M_A$ when its MAP estimate and the MLE estimate are equal and likewise for $M_B$ by symmetry. 
\section{Practical problems}
\subsection{Spherical Gaussian estimation}
\paragraph{(a)}
\begin{align*}
\boldsymbol\mu_\mathrm{\scriptscriptstyle{MLE}}&=\frac{1}{n}\sum_{i=1}^n\mathbf{x_i}\\
\sigma_\mathrm{\scriptscriptstyle{MLE}}&=\sqrt{\frac{1}{np}\sum_{i=1}^n(\mathbf{x_i}-\boldsymbol\mu_\mathrm{\scriptscriptstyle{MLE}})^\top(\mathbf{x_i}-\boldsymbol\mu_\mathrm{\scriptscriptstyle{MLE}})}.
\end{align*}
\paragraph{(d)}
\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=\textwidth]{P2_1}
  \end{center}
\end{figure}
\subsection{MAP estimation}
\end{document}
