\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}

\DeclareMathOperator{\given}{\mid}

\begin{document}

\section*{HW2}

\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill} } lll}
Jimmy Hold\"{o} & & Jared Karr\\
890130-6319 & & 801120-4693\\
\it{gusholji@student.gu.se} & & \it{karr@student.chalmers.se}\\
\end{tabular*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayes classifer}
We observe the following probabilities:
\begin{align*}
&&P(c=1)&=1/2 & &\\
P(x_1=0)&=1/2 &&& P(x_1=0\given c=1)&=1/4\\
P(x_2=1)&=3/8 &&& P(x_2=1\given c=1)&=1/2\\
P(x_3=1)&=1/2 &&& P(x_3=1\given c=1)&=3/4
\end{align*}

\paragraph{(a)}
\begin{align*}
  P(c=1\given x=(0, 1, 1))
  &=\frac{
    P(x_1=0\given c=1)
    P(x_2=1\given c=1)
    P(x_3=1\given c=1)
    P(c=1)
  }{
    P(x_1=0)
    P(x_2=1)
    P(x_3=1)
  }\\
  &=\frac{
    (1/4)(1/2)(3/4)(1/2)
  }{
    (1/2)(3/8)(1/2)
  }=1/2
\end{align*}

\paragraph{(b)}
\begin{align*}
  P(c=1\given x=(0, 1, 1))
  &=\frac{
    P(x_1=0\given c=1)
    P(x_2=1\given c=1)
    P(c=1)
  }{
    P(x_1=0)
    P(x_2=1)
  }\\
  &=\frac{
    (1/4)(1/2)(1/2)
  }{
    (1/2)(3/8)
  }=1/3
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extending na\"ive Bayes}
Features $x_1$, $x_2$, and $x_3$ are not independent. In fact, they are strictly dependent upon each other. The na\"ive Bayes method can be applied to the same data by changing the feature definition:
\begin{align*}
  x_{123} &= \left\{
    \begin{matrix*}[l]
      0, & & \textrm{if customer is younger than 20}\\
      1, & & \textrm{if customer is between 20 and 30}\\
      2, & & \textrm{if customer is older than 30}
    \end{matrix*}
  \right.\\
  x_4 &= \left\{
    \begin{matrix*}[l]
      0, & & \textrm{if customer does not walk to work}\\
      1, & & \textrm{otherwise}
    \end{matrix*}
  \right.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Practical problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayes classifier}
\paragraph{(a)}
The Bayes classifier for a 3-dimensional spherical Gaussian model having two classes $k\in\{-1,1\}$ with a uniform prior is
\begin{align*}
  P( y_\mathrm{\scriptscriptstyle{new}}=1
    \given \mathbf{x}_\mathrm{\scriptscriptstyle{new}}, \mathcal{D})
  &=
  \frac{
    \mathcal{N}( \mathbf{x}_\mathrm{\scriptscriptstyle{new}}
      \given \boldsymbol{\hat\mu}_1, \hat\sigma_1^2)
  }{
    \mathcal{N}( \mathbf{x}_\mathrm{\scriptscriptstyle{new}}
      \given \boldsymbol{\hat\mu}_1, \hat\sigma_1^2)
    +
    \mathcal{N}( \mathbf{x}_\mathrm{\scriptscriptstyle{new}}
      \given \boldsymbol{\hat\mu}_{-1}, \hat\sigma_{-1}^2)
  }
  \\
%%%%%
  &=
  \frac{1}{
    1 + \frac{
      \mathcal{N}( \mathbf{x}_\mathrm{\scriptscriptstyle{new}}
        \given \boldsymbol{\hat\mu}_{-1}, \hat\sigma_{-1}^2)
    }{
      \mathcal{N}( \mathbf{x}_\mathrm{\scriptscriptstyle{new}}
        \given \boldsymbol{\hat\mu}_1, \hat\sigma_1^2)
    }
  }
  \\
%%%%%
  &=
  \frac{1}{
    1 + \left(\frac{
    \hat\sigma_1
    }{
    \hat\sigma_{-1}
    }\right)^3
    \exp\left(
      \frac{
        (\mathbf{x}_\mathrm{\scriptscriptstyle{new}}-\boldsymbol{\hat\mu}_{-1})^\top
        (\mathbf{x}_\mathrm{\scriptscriptstyle{new}}-\boldsymbol{\hat\mu}_{-1})
      }{\hat\sigma_{-1}^2}
      -\frac{
        (\mathbf{x}_\mathrm{\scriptscriptstyle{new}}-\boldsymbol{\hat\mu}_{1})^\top
        (\mathbf{x}_\mathrm{\scriptscriptstyle{new}}-\boldsymbol{\hat\mu}_{1})
      }{\hat\sigma_{1}^2}
    \right)
  }.
  \\
%%%%%
\intertext{If $\hat\sigma_{-1}^2=\hat\sigma_{1}^2$, then this simplifies to a logistic function:}
%%%%%
  &=
  \frac{1}{
    1 + \exp\left(
      \frac{
        2(\boldsymbol{\hat\mu}_{-1}-\boldsymbol{\hat\mu}_1)^\top\mathbf{x}_\mathrm{\scriptscriptstyle{new}}
        + \boldsymbol{\hat\mu}_{-1}^\top\boldsymbol{\hat\mu}_{-1}
        - \boldsymbol{\hat\mu}_{1}^\top\boldsymbol{\hat\mu}_{1}
      }{\hat\sigma^2}
    \right)
  },
  \\
%%%%%
\intertext{By symmetry,}
%%%%%
  P( y_\mathrm{\scriptscriptstyle{new}}=-1
    \given \mathbf{x}_\mathrm{\scriptscriptstyle{new}}, \mathcal{D})
  &=
  \frac{1}{
    1 + \left(\frac{
      \hat\sigma_{-1}
    }{
      \hat\sigma_{1}
    }\right)^3
  \exp\left(
    \frac{
      (\mathbf{x}_\mathrm{\scriptscriptstyle{new}}-\boldsymbol{\hat\mu}_{1})^\top
      (\mathbf{x}_\mathrm{\scriptscriptstyle{new}}-\boldsymbol{\hat\mu}_{1})
    }{\hat\sigma_{1}^2}
    -\frac{
      (\mathbf{x}_\mathrm{\scriptscriptstyle{new}}-\boldsymbol{\hat\mu}_{-1})^\top
      (\mathbf{x}_\mathrm{\scriptscriptstyle{new}}-\boldsymbol{\hat\mu}_{-1})
    }{\hat\sigma_{-1}^2}
  \right)
}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Handwritten digit recognition}
\end{document}
